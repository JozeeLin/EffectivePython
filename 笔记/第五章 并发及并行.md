# 第5章 并发及并行

> 并发(concurrency)的意思是说,计算机似乎是在同一时间做着很多不同的事.但其实是操作系统在各程序之间迅速切换,使其都有机会运行在这一个处理器上面.
>
> 并行(parallelism)的意思则是说,计算机确实是在同一时间做着很多不同的事.
>
> 在同一个程序内部,并发是一种工具,它使程序员可以更加方便的解决特定类型的问题.在并发程序中,不同的执行路径都能够以某种方式向前推进,而这种方式,使人感觉那些路径可以在同一时间独立地运行.
>
> 并行与并发的关键区别,就在于能不能提速.某程序若是并行程序,其中有两条不同的执行路径都在平行的向前推进,则总任务的执行时间会减半,执行速度会变为普通程序的两倍.反之,假如该程序是并发程序,那么它即使可以用看似平行的方式分别执行多条路径,也依然不会使总任务的执行速度得到提升.

## 第36条: 用subprocess模块来管理子进程

> 由python所启动的多个子进程,是可以平行运作的,这使得我们能够在python程序中充分利用电脑中的全部CPU核心,从而尽量提成程序的处理能力(throughput,吞吐量).虽然python解释器本事可能会受限于CPU(参见本书第37条),但是开发者依然可以用python顺畅的驱动并协调那些耗费CPU的工作任务.

```python
# 用Popen构造器来启动进程
proc = subprocess.Popen(['echo', 'Hello from the child!'],stdout=subprocess.PIPE)
out, err = proc.communicate() # 用communicate方法读取子进程的输出信息,并等待其终止
print(out.decode('utf-8'))

# 子进程独立于父进程运行;父进程为python解释器;一边定期查询子进程的状态,一边处理其他事务
proc = subprocess.Popen(['sleep','0.3'])
while proc.poll() is None:
    print('Working...')
    
print('Exit status', proc.poll())

'''
把子进程从父进程中剥离(decouple,解耦),意味着父进程可以随意运行很多条平行的子进程.为了实现这一点,我们可以先把所有的子进程都启动起来.
'''
def run_sleep(period):
    proc = subprocess.Popen(['sleep', str(period)])
    return proc

start = time()
procs = []
for _ in range(10):
    proc = run_sleep(0.1)
    procs.append(proc)
    
#然后通过communicate方法,等待这些子进程完成其I/O工作并终结
for proc in procs:
    proc.communicate()
end = time()
print('Finished in %.3f seconds' % (end-start))
'''
以上代码的结论:父进程会把所有的子进程都启动起来,而不是把这些子进程逐个运行,因为如果是逐个运行的化,完成时间应该是1秒左右,而不是0.1秒左右.
'''

'''
我们可以从python程序项子进输送数据,或者获取子进程的输出信息.例如:用命令行工具openssl加密一些数据
'''
def run_openssl(data):
    env = os.environ.copy()
    env['password'] = b'\xe24U\n\xd0Q13S\x11'
    proc = subprocess.Popen(
        ['openssl','enc','-des3','-pass','env:password'],
        env=env,
        stdin=subprocess.PIPE,
        stdout=subprocess.PIPE
    )
    proc.stdin.write(data)
    proc.stdin.flush()  # Ensure the child gets input
    return proc

procs = []
for _ in range(3):
    data = os.urandom(10)
    proc = run_openssl(data)
    procs.append(proc)
    
for proc in procs:
    out, err = proc.communicate()
    print(out[-10:])
    
'''
另外我们还可以像UNIX管道那样,把子进程搭建链条(chain),即把第一个子进程的输出与第二个子进程的输入联系起来,并以此方式继续拼接下去.
'''
def run_md5(input_stdin):
    proc = subprocess.Popen(
        ['md5sum'],
        stdin = input_stdin,
        stdout=subprocess.PIPE
    )
    return proc
'''
现在启动一套openssl进程,以便加密某些数据,同时启动另一套md5进程,以便根据加密后的输出内容来计算其哈希码
'''
input_procs = []
hash_procs = []
for _ in range(3):
    data = os.urandom(10)
    proc = run_openssl(data)
    input_procs.append(proc)
    hash_proc = run_md5(proc.stdout) #使用上一个进程的输出作为当前进程的输入
    hash_procs.append(hash_proc)

for proc in input_procs:
    proc.communicate()

for proc in hash_procs:
    out, err = proc.communicate()
    print(out.strip())
    
'''
    如果担心子进程一直不终止,或担心它的输出管道或输入管道由于某些原因发生阻塞,那么可以给communicate方法传入timeout参数.
    若子进程在指定时间内没有给出响应,那么communicate会抛出异常,我们可以在处理异常的时候,终止出现意外的子进程
'''
proc = run_sleep(10)
try:
    proc.communicate(timeout=0.1)
except subprocess.TimeoutExpired:
    proc.terminate()
    proc.wait()
print('Exit status', proc.poll()) #输出子进程的退出状态
```



> timeout参数仅在python3.3及后续版本中有效.对于之前的版本,我们需要用内置的select模块来处理proc.stdin,proc.stdout,proc.stderr以确保I/O操作的超时机制能够生效.



### 要点

- 可以用subprocess模块运行子进程,并管理其输入流与输出流
- python解释器能够平行的运行多个子进程,这使得开发折可以充分利用cpu的处理能力
- 可以给communicate方法传入timeout参数,以避免子进程死锁或失去响应(hanging,挂起)

## 第37条: 可以用线程来执行阻塞式I/O,但不要用它做平行计算

> 标准的python实现叫做Cpython.CPython分两步来运行python程序:
>
> 首先,把文本形式的源代码解析并编译成字节码.
>
> 然后,用一种基于栈的解释器来运行这份字节码.
>
> 执行python程序时,字节码解释器必须保持协调一致的状态.python采用GIL(global interpreter lock,全局解释器锁)机制来确保这种同步性.
>
> GIL实际上是一把互斥锁(mutex, 互斥体),用以防止CPython受到占先式多线程切换操作的干扰.
>
> 所谓占先式多线程切换,是指某个线程可以通过打断另外一个线程的方式,来获取程序控制权.
>
> 而GIL可保证不会出现这种打断的操作,使得每条字节码指令均能够正确的与CPython实现及其C语言扩展模块协同运作.
>
> GIL的缺陷:用C++或Java编程的时候,可以同时执行多条线程,以充分利用计算机所配备的多个CPU核心.Python程序尽管也支持多线程,但由于受到GIL保护,所以同一时刻,只有一条线程可以向前执行.也就是说在多核计算机上无法实现多线程并行计算.

```python
def factorize(number):
    for i in range(1, number+1):
        if number % i == 0:
            yield i
            
numbers = [2139079, 1214759, 1516637, 1852285]
start = time()
for number in numbers:
    list(factorize(number))
end = time()
print('Took %.3f seconds' % (end-start))

'''
使用python的多线程库
'''
from threading import Thread
class FactorizeThead(Thread):
    def __init__(self, number):
        super().__init__()
        self.number = number
        
    def run(self):
        self.factors = list(factorize(self.number))
        
start = time()
threads = []
for number in numbers:
    thread = FactorizeThread(number)
    thread.start()
    threads.append(thread)

for thread in threads:
    thread.join()

end = time()
print('Took %.3f seconds' % (end-start))
'''
通过以上两种不同的实现,我们可以发现消耗的时间相差不多,甚至使用多线程的实现方式耗费的时间比第一种方式稍微多一点点.

结论:CPython解释器中的多线程程序受到了GIL的影响.

通过一些其他方式,我们确实可以令CPython解释器利用CPU的多个内核,但是那些方式所使用的并不是标准的Thread类(参见本书的第41条),而且还需要开发者编写较多的代码.那么python的多线程机制的作用是什么呢?

其一,多线程使得程序看上去好像能够在同一时间做许多事情.
其二,多线程可用于处理阻塞式I/O操作,Python在执行某些系统调用时,会触发此类操作.执行系统调用,是指Python程序请求计算机的操作系统与外界环境相交互,以满足程序的需求.
'''

'''
举例:通过串行端口(serial port,简称串口)发送信号,以便远程控制一架直升飞机.以下使用系统调用select来模拟这项活动;该函数请求操作系统阻塞0.1秒,然后把控制权还给程序,这种效果与通过同步串口来发送信号是类似的.
'''
import select
def slow_systemcall():
    select.select([],[],[],0.1)

start = time()
for _ in range(5):
    slow_systemcall() #此处程序的主线程会卡在select系统调用那里.
end = time()
print('Took %.3f seconds' % (end-start))
'''
以上代码的致命缺点:因为发送信号的同时,程序必须算出直升飞机接下来要移动到的地点,否则肺经可能就会撞毁.如果要同时执行阻塞式I/O操作和计算操作,那就应该考虑把系统调用放到其他线程里面.
'''
def compute_helicopter_location(index):
    pass

for i in range(5):
    compute_helicopter_location(i)
    
for thread in threads:
    thread.join()
end = time()
print('Took %.3f seconds' % (end-start))
'''
除了线程,还有其他一些方式,也能处理阻塞式的I/O操作,例如,内置的asyncio模块等.虽然那些方式都有着非常显著的有点,但它们要求开发者必须花些功夫,将代码重构成另外一种执行模型(参见第40条).
'''
```

### 要点

- 因为受到全局解释锁(GIL)的限制,所以多条Python线程不能在多个CPU核心上面平行地执行代码
- 尽管受制于GIL,但是Python的多线程功能依然很有用,它可以轻松的模拟出同一时刻执行多项任务的效果
- 通过Python线程,我们可以平行地执行多个系统调用,这使得程序能够在执行阻塞式I/O操作的同时,执行一些运算操作.

## 第38条: 在线程中使用Lock来方式数据竞争

> GIL并不会保护开发者自己所编写的代码.同一时刻固然只能有一个python线程得以运行,但是,当这个线程正在操作某个数据结构时,其他线程可能会打断它,也就是说,python解释器在执行两个连续的字节码指令时,其他线程可能会在中途突然插进来.如果开发者尝试从多个线程中同时访问某个对象,那么上述情形就会引发危险的结果.

```python
class Counter(object):
    def __init__(self):
        self.count = 0
        
    def increment(self, offset):
        self.count += offset
        
        
# 查询传感器读数的时候会发生阻塞式I/O操作,所以要给每个传感器分配自己的工作线程.每采集一次读数,工作线程就会给Counter对象的value值加1,然后继续采集,直至完成全部采样操作
def worker(sensor_index, how_many, counter): #how_many表示样本总数
    for _ in range(how_many):
        counter.increment(1)
        
#为每个传感器启动一条工作线程,然后等待它们完成各自的采样工作
def run_threads(func, how_many, counter):
    threads=[]
    for i in range(5):
        args = (i, how_many, counter)
        thread = Thread(target=func, args=args)
        threads.append(thread)
        thread.start()
    for thread in threads:
        thread.join()
        
how_many = 10**5
counter = Counter()
run_threads(worker, how_many, counter)
print('Counter should be %d, found %d' % (5*how_many, counter.count))
'''
最终的结构不是50万,而是少于50万;
原因:为了保证所有的线程都能够公平地执行,GIL会给每个线程分配大致相等的处理器时间.那么有可能出现有一些线程正在执行的时候被暂停(suspend),那么有些计数操作并不是原子操作,当计数操作没有完成之前被暂停,就会出现以上的情况.

+=操作符,实际上GIL会把它拆解成3个独立的操作来完成的:
1. value = getattr(counter, 'count)
2. result = value+offset
3. setattr(counter, 'count', result)

而线程有可能在任意两个操作之间发生线程切换.这种交错执行方式会令线程把旧的value设置给Counter,从而使程序的运行结果出现问题.
比如,运行线程A和线程B,它们的线程切换情况如下:
# Running in Thread A
value_a = getattr(counter, 'count')
#Context switch to Thread B
value_b = value_b +1
setattr(counter, 'count', value_b)
# Context switch back to Thread A
result_a = value_a + 1
setattr(counter, 'count', result_a)
上面的例子中,当线程B执行结束之后,切换回线程A的时候,线程A把线程B刚才对计数器所做的递增效果完全抹去了(这种现象叫做线程A踩踏(stomp)线程B).

为了防止诸如此类的竞态,threading模块提供了一套健壮的工具,使得开发者可以保护自己的数据结构不受破坏.其中最简单的就是Lock类,该类相当于互斥锁
'''
class LockingCounter(object):
    def __init__(self):
        self.lock = Lock()
        self.count = 0
        
    def increment(self, offset):
        with self.lock:
            self.count += offset
```

### 要点

-  python确实有全局解释器锁,但是在编写自己的程序时,依然要设法防止多个线程争用一份数据
- 如果在不加锁的前提下,允许多条线程修改同一个对象,那么程序的数据结构可能会遭到破坏
- 在python内置的threading模块中,有个名叫Lock的类,它用标准的方式实现了互斥锁

## 第39条: 用Queue来协调各线程之间的工作

> 如果python程序同时要执行许多事务,那么开发者经常需要协调这些事务.而较为高效的协调方式为采用函数管道(pipeline).

```python
'''
构建一个照片处理系统,该系统从数码相机里面持续获取照片,调整其尺寸,并将其添加到网络相册中.系统设计:
1. 第一阶段获取新图片
2. 第二阶段把下载好的图片传给缩放函数
3. 第三阶段把缩放后的图片交给上传函数
如何这三个阶段拼接为一条可以并发处理照片的管线呢?
首先,设计一种任务传递方式,以便在管线的不同阶段之间传递工作任务.可以用线程安全的生产者-消费者队列来建模(线程安全的重要性请参阅第38条,deque类的用法请参阅本书第46条)
'''
class MyQueue(object):
    def __init__(self):
        self.items = deque()
        self.lock = Lock()
        
    def put(self, item):
        '''这个代表数码相机,相当于生产者'''
        with self.lock:
            self.items.append(item)
            
    def get(self):
        '''图片处理管线的第一阶段-获取图片,相当于消费者'''
        with self.lock:
            return self.items.popleft()
'''
接下来,我们使用python线程来表示管线的各个阶段
'''
class Worker(Thread):
    def __init__(self, func, in_queue, out_queue):
        super().__init__()
        self.func = func
        self.in_queue = in_queue
        self.out_queue = out_queue
        self.polled_count = 0
        self.work_done = 0
        
    def run(self):
        while True:
            self.polled_count += 1
            try:
                item = self.in_queue.get()
            except IndexError:
                sleep(0.01)  # No work to do
            else:
                result = self.func(item)
                self.out_queue.put(result)
                self.work_done += 1
                
download_queue = MyQueue()
resize_queue = MyQueue()
upload_queue = MyQueue()
done_queue = MyQueue()
threads = [
    Worker(download, download_queue, resize_queue),
    Worker(resize, resize_queue, upload_queue),
    Worker(upload, upload_queue, done_queue),
]
for thread in threads:
    thread.start()
for _ in range(1000):
    download_queue.put(object())
    
while len(done_queue.items)<1000:
    pass
'''
问题:
1. 上面的worker类的run方法,会定时的查询输入队列,如果上一阶段迟迟没有完成,那么会造成后面阶段的工作线程会处于饥饿(starve)状态,从而使得工作线程白白浪费CPU实现去执行一些没有用的操作.

2. 另外,为了判断所有任务是否都彻底处理完毕,我们必须再编写一个循环,持续判断done_queue队列中的任务数量.

3. 其次,woker线程的run方法会一直执行循环,即使完成了所有的任务,也无法退出循环.

4. 如果我们要处理的任务非常多,而多个阶段之间的处理速度不一致的情况下,如果第一阶段的处理速度非常快,而第二阶段处理速度非常慢,那么第二阶段的输入队列会占用越来越多的内存,从而导致内存泄露.

使用queue类来弥补自编队列的缺陷;该类能够彻底解决上面提出的那些问题.
1. 无需频繁查询输入队列的状态,因为它的get方法会持续阻塞,直到有新的数据加入
2. 为了解决管线的迟滞问题,用Queue类来限定队列中待处理的最大任务数量,使得相邻的两个阶段,可以通过该队列平滑地衔接起来.因为构造Queue时,可以指定缓冲区的容量,如果队列已满,那么后续的put方法会阻塞.
3. 通过Queue类的task_done的方法来追踪工作进度.有了这个方法,我们就不用像原来那样,进行轮询判断任务是否结束.即使用Queue类的join方法等待队列任务结束.
4. 通过在队列中添加一个特殊的对象,来判断是否到队列尾了.
'''
from queue import Queue
queue = Queue()

def consumer():
    print('Consumer waiting')
    queue.get()
    print('Consumer done')
    
thread = Thread(target=consumer)
thread.start()

print('Producer putting')
queue.put(object())
thread.join()
print('Producer done')

#设置队列缓冲区容量
queue = Queue(1)
def consumer():
    time.sleep(0.1) #给生产者流出一些时间
    queue.get()
    print('Consumer got 1') # run 2
    queue.get()
    print('Consumer got 2') # run 4
    
thread = Thread(target=consumer)
thread.start()

queue.put(object())
print('Product put 1') # run 1
queue.put(object())
print('Product put 2') # run 3
thread.join()
print('Product done')

# 追踪工作进度
in_queue = Queue()
def consumer():
    print('Consumer waiting')
    work = in_queue.get()
    print('Consumer working')
    print('working ...')
    print('Consumer done')
    in_queue.task_done()
    
Thread(target=consumer).start()

in_queue.put(object())
print('Producer waiting')
in_queue.join()
print('Producer done')

# 使用Queue来实现
def download(item):
    print('download:',item)


def resize(item):
    print('resize:',item)


def upload(item):
    print('upload:', item)


class ClosableQueue(Queue):
    SENTINEL = object()

    def close(self):
        self.put(self.SENTINEL)

    def __iter__(self):
        while True:
            item = self.get()
            try:
                if item is self.SENTINEL:
                    return # Cause the thread to exit
                yield item
            finally:
                self.task_done()


class StoppableWorker(Thread):
    '''生产者'''
    def __init__(self,func,in_queue,out_queue):
        super().__init__()
        self.func = func
        self.in_queue = in_queue
        self.out_queue = out_queue
        self.polled_count = 0
        self.work_done = 0


    def run(self):
        for item in self.in_queue:
            result = self.func(item)
            self.out_queue.put(result)


download_queue = ClosableQueue()
resize_queue = ClosableQueue()
upload_queue = ClosableQueue()
done_queue = ClosableQueue()

threads = [
    StoppableWorker(download, download_queue, resize_queue),
    StoppableWorker(resize, resize_queue, upload_queue),
    StoppableWorker(upload, upload_queue, done_queue),
]

for thread in threads:
    thread.start()

for _ in range(10):
    download_queue.put(object())

download_queue.close()
download_queue.join()
resize_queue.close()
resize_queue.join()
upload_queue.close()
upload_queue.join()
print(done_queue.qsize(), 'items finished')
```

### 要点

- 管线是一种优秀的任务处理方式,它可以把处理流程划分为若干阶段,并使用多余python线程来同时执行这些任务
- 构建并发式的管线时,要注意许多问题,其中包括:如何防止某个阶段陷入持续等待的状态之中,如何停止工作线程,以及如何防止内存膨胀等.
- Queue类所提供的机制,可以彻底解决上述问题,它具备阻塞式的队列操作,能够指定缓冲区尺寸,而且还支持join方法,这使得开发者可以构建出健壮的管线.

## 第40条: 考虑用协程来并发地运行多个函数

> 线程有三个显著的缺点:
>
> 1. 为了确保数据安全,我们必须使用特殊的工具来协调这些线程,这使得多线程代码比单线程代码更难懂.使得程序难以维护以及扩展
> 2. 线程需要占用大量内存,每个正在执行的线程,大约占据8MB内存.
> 3. 线程启动时的开销比较大.如果程序不停的依靠创建新线程来同时执行多个函数,并等待这些线程结束,那么使用线程所引发的开销,会拖慢整个程序的速度.
>
> python的coroutine(协程)可以避免上述问题,它使得python程序看上去好像在同时运行多个函数.协程的实现方式,实际上是对生成器的一种扩展.启动生成器协程所需的开销,与调用函数的开销相仿.处于活跃状态的协程,在其耗尽之前,只会占用不到1KB的内存.
>
> 协程的工作原理:每当生成器函数执行到yield表达式的时候,消耗生成器的那段代码,那就通过send方法给生成器回传一个值.而生成器在收到经由send函数所传进来的这个值之后,将其视为yield表达式的执行结果.